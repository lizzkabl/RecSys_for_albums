{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "165idl3hYaAF"
   },
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwXPz7bDYWAG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbTB3-uBYeqy"
   },
   "source": [
    "# Обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-iRm4rWZJmF",
    "outputId": "bc6bb831-e3cd-4614-b91b-bb102cecd9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pes6UkSzZKjD"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/content/drive/MyDrive/spotify_albums.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDtut9WjbQqQ"
   },
   "outputs": [],
   "source": [
    "data.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlHezDzoZixp",
    "outputId": "7abc1c5d-b516-42b1-a7e6-dca35f16c628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   album_name           200 non-null    object \n",
      " 1   artist               200 non-null    object \n",
      " 2   date                 200 non-null    object \n",
      " 3   duration             200 non-null    float64\n",
      " 4   amount_of_element    200 non-null    int64  \n",
      " 5   genre                200 non-null    object \n",
      " 6   popularity           200 non-null    int64  \n",
      " 7   amount_of_listeners  200 non-null    int64  \n",
      " 8   type_of_album        200 non-null    object \n",
      " 9   featured_artists     25 non-null     object \n",
      " 10  artist_followers     200 non-null    int64  \n",
      " 11  label                200 non-null    object \n",
      "dtypes: float64(1), int64(4), object(7)\n",
      "memory usage: 18.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2Q625pGYrFc"
   },
   "source": [
    "# Метод на основе похожести"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuBEZHTVbHFb",
    "outputId": "1dcf888c-6e5d-4efb-faa9-027aad6dcca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рекомендуемые альбомы: ['ROCK-STAR', 'ATE', 'GIANT', 'rosie', 'Unorthodox Jukebox']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "data['combined_features'] = (\n",
    "    data['artist'].astype(str) + ' ' +\n",
    "    data['date'].astype(str) + ' ' +\n",
    "    data['duration'].astype(str) + ' ' +\n",
    "    data['amount_of_element'].astype(str) + ' ' +\n",
    "    data['genre'].astype(str) + ' ' +\n",
    "    data['popularity'].astype(str) + ' ' +\n",
    "    data['amount_of_listeners'].astype(str) + ' ' +\n",
    "    data['type_of_album'].astype(str) + ' ' +\n",
    "    data['featured_artists'].astype(str) + ' ' +\n",
    "    data['artist_followers'].astype(str) + ' ' +\n",
    "    data['label'].astype(str)\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(data['combined_features'])\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "def recommend_albums(album_title):\n",
    "    try:\n",
    "        idx = data[data['album_name'] == album_title].index[0]\n",
    "    except IndexError:\n",
    "        return \"Альбом не найден.\"\n",
    "\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sim_indices = [i[0] for i in sim_scores[1:6]]\n",
    "\n",
    "    return data['album_name'].iloc[sim_indices].tolist()\n",
    "\n",
    "\n",
    "recommended = recommend_albums('HOP')\n",
    "print(\"Рекомендуемые альбомы:\", recommended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0R9rN0nEr8q"
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H67j9GtxCoAU",
    "outputId": "f0e264f2-f112-4cfe-e081-67cd2e3c445e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рекомендуемые альбомы:\n",
      "['ATE', 'ROCK-STAR', 'rosie', 'APT.', 'Good Luck, Babe!', 'GIANT', 'FERXXOCALIPSIS', \"Short n' Sweet\", 'BRAT']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Создаем DataFrame\n",
    "df = data\n",
    "\n",
    "# Объединяем текстовые признаки в один столбец\n",
    "df['text_features'] = df['genre'] + ' ' + df['artist'] + ' ' + df['type_of_album'] + ' ' + df['label']\n",
    "\n",
    "# Векторизация текстовых данных\n",
    "vectorizer = TfidfVectorizer()\n",
    "text_matrix = vectorizer.fit_transform(df['text_features'])\n",
    "\n",
    "# Нормализация числовых признаков\n",
    "scaler = StandardScaler()\n",
    "numeric_features = scaler.fit_transform(df[['duration', 'popularity', 'amount_of_element', 'artist_followers', 'amount_of_listeners']])\n",
    "\n",
    "# Объединяем текстовые и числовые признаки и преобразуем в CSR формат\n",
    "features = csr_matrix(hstack([text_matrix, numeric_features]))\n",
    "\n",
    "# Обучаем модель k-NN\n",
    "knn = NearestNeighbors(n_neighbors=10, metric='cosine')  # Используем косинусное расстояние\n",
    "knn.fit(features)\n",
    "\n",
    "# Функция для получения рекомендаций\n",
    "def get_recommendations(album_name, knn_model=knn):\n",
    "    # Находим индекс альбома\n",
    "    idx = df[df['album_name'] == album_name].index\n",
    "    if len(idx) == 0:\n",
    "        return \"Альбом не найден\"\n",
    "\n",
    "    idx = idx[0]\n",
    "\n",
    "    # Получаем вектор признаков для этого альбома (используем CSR формат)\n",
    "    album_features = features[idx, :].toarray()  # Преобразуем в плотный формат\n",
    "\n",
    "    # Находим k ближайших соседей\n",
    "    distances, indices = knn_model.kneighbors(album_features)\n",
    "\n",
    "    # Возвращаем названия рекомендованных альбомов (исключая сам альбом)\n",
    "    recommendations = df['album_name'].iloc[indices[0][1:]].tolist()\n",
    "    return recommendations\n",
    "\n",
    "# Пример использования\n",
    "recommendations = get_recommendations('HOP')\n",
    "print(\"Рекомендуемые альбомы:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJ9i3zjXvj2V"
   },
   "source": [
    "# Автоэнкодер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSCju45CvrTq",
    "outputId": "8bb38fcf-0c5b-4419-fc32-c7d105f66400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   THE TORTURED POETS DEPARTMENT\n",
      "1                         1989 (Taylor's Version)\n",
      "2                1989 (Taylor's Version) [Deluxe]\n",
      "3                                Starboy (Deluxe)\n",
      "4    THE TORTURED POETS DEPARTMENT: THE ANTHOLOGY\n",
      "Name: album_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# === Загрузка данных ===\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/spotify_albums.csv\")\n",
    "\n",
    "# === Предобработка ===\n",
    "df = df.drop(columns=[\"album_name\", \"featured_artists\", \"label\", \"date\"])\n",
    "# df[\"year\"] = pd.to_datetime(df[\"date\"]).dt.year\n",
    "# df = df.drop(columns=[\"date\"])\n",
    "\n",
    "categorical_cols = [\"artist\", \"genre\", \"type_of_album\"]\n",
    "for col in categorical_cols:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df)\n",
    "\n",
    "# === Создание датасета ===\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "dataset = TensorDataset(X_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=4):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, embed_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "model = Autoencoder(X.shape[1], embed_dim=4)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# === Обучение ===\n",
    "for epoch in range(100):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# === Получение эмбеддингов ===\n",
    "with torch.no_grad():\n",
    "    embeddings = model.encoder(X_tensor).numpy()\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def recommend_by_album_list_advanced(album_names, top_n=5):\n",
    "    # Найдём индексы указанных альбомов\n",
    "    indices = data[data[\"album_name\"].isin(album_names)].index.tolist()\n",
    "\n",
    "    if not indices:\n",
    "        raise ValueError(\"Ни один из указанных альбомов не найден.\")\n",
    "\n",
    "    # Чем ближе к последнему, тем больший вес (затухание слева направо)\n",
    "    weights = np.exp(np.linspace(-1.0, 0.0, num=len(indices)))  # например: [0.37, 0.60, 1.0]\n",
    "    weights /= weights.sum()  # нормализация\n",
    "\n",
    "    # Взвешенное усреднение\n",
    "    weighted_embedding = np.average(embeddings[indices], axis=0, weights=weights)\n",
    "\n",
    "    # Косинусное сходство\n",
    "    sims = cosine_similarity([weighted_embedding], embeddings)[0]\n",
    "\n",
    "    # Исключим исходные альбомы\n",
    "    for idx in indices:\n",
    "        sims[idx] = -1\n",
    "\n",
    "    # Получаем top-N\n",
    "    top_indices = sims.argsort()[::-1][:top_n]\n",
    "\n",
    "    return data.loc[top_indices, \"album_name\"].reset_index(drop=True)\n",
    "\n",
    "# Пример:\n",
    "recent_likes = [\"Hurry Up Tomorrow\", \"Scorpion\", \"After Hours\"]\n",
    "recommendations = recommend_by_album_list_advanced(recent_likes)\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "la0K3Tzq5YRv"
   },
   "source": [
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Q9aw43F5jns",
    "outputId": "e3777e83-6744-4355-b62b-1ed87eac2369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-tabnet-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYFPeGFM5X8s",
    "outputId": "a2068d06-8820-4720-a81b-9b9aeabec841"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.81496 | val_0_rmse: 52.3016 |  0:00:00s\n",
      "epoch 1  | loss: 0.69405 | val_0_rmse: 40.69386|  0:00:00s\n",
      "epoch 2  | loss: 1.2221  | val_0_rmse: 10.73387|  0:00:00s\n",
      "epoch 3  | loss: 1.05652 | val_0_rmse: 13.11426|  0:00:00s\n",
      "epoch 4  | loss: 0.41439 | val_0_rmse: 13.73422|  0:00:00s\n",
      "epoch 5  | loss: 0.3133  | val_0_rmse: 12.60007|  0:00:00s\n",
      "epoch 6  | loss: 0.48972 | val_0_rmse: 11.35278|  0:00:00s\n",
      "epoch 7  | loss: 0.34577 | val_0_rmse: 11.00366|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 7 with best_epoch = 2 and best_val_0_rmse = 10.73387\n",
      "Рекомендуемые альбомы:\n",
      "                  album_name  artist  genre  popularity\n",
      "11                  Her Loss      37     10    1.000000\n",
      "27     Unapologetic (Deluxe)      56      0    0.882353\n",
      "125  Funk Wav Bounces Vol. 2      21     16    0.764706\n",
      "172                      HOP      17      5    0.764706\n",
      "148                    DRIVE      25     16    0.705882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Загрузка данных\n",
    "\n",
    "\n",
    "# Обработка данных\n",
    "data = data.drop_duplicates(subset=[\"album_name\", \"artist\"])\n",
    "data[\"genre\"] = data[\"genre\"].fillna(\"Unknown\")\n",
    "\n",
    "# Кодируем категориальные признаки\n",
    "cat_features = [\"artist\", \"genre\", \"type_of_album\", \"label\"]\n",
    "for col in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "# Нормализуем числовые признаки\n",
    "num_features = [\"duration\", \"amount_of_element\", \"popularity\", \"amount_of_listeners\", \"artist_followers\"]\n",
    "scaler = MinMaxScaler()\n",
    "data[num_features] = scaler.fit_transform(data[num_features])\n",
    "\n",
    "# Создаем матрицу признаков\n",
    "features = data.drop(columns=[\"album_name\", \"date\", \"featured_artists\"])\n",
    "album_names = data[\"album_name\"].values\n",
    "\n",
    "# Создаем искусственную целевую переменную (имитация рейтингов)\n",
    "np.random.seed(42)\n",
    "data[\"target\"] = np.random.uniform(0, 1, size=len(data))\n",
    "\n",
    "# Разделяем данные\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, data[\"target\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Конвертируем в numpy\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# Инициализация и обучение TabNet\n",
    "tabnet = TabNetRegressor(\n",
    "    n_d=32,\n",
    "    n_a=32,\n",
    "    n_steps=3,\n",
    "    gamma=1.3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type=\"sparsemax\",\n",
    "    device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "tabnet.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=['rmse'],\n",
    "    max_epochs=30,\n",
    "    patience=5,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=64,\n",
    "    drop_last=False,\n",
    "    augmentations=None\n",
    ")\n",
    "\n",
    "# Функция для получения рекомендаций\n",
    "def get_tabnet_recommendations(liked_albums, n_recommendations=5):\n",
    "\n",
    "    # Получаем индексы понравившихся альбомов\n",
    "    liked_indices = [np.where(album_names == album)[0][0] for album in liked_albums\n",
    "                    if album in album_names]\n",
    "\n",
    "    if not liked_indices:\n",
    "        return pd.DataFrame(columns=[\"album_name\", \"artist\", \"genre\", \"popularity\"])\n",
    "\n",
    "    # Получаем предсказания для всех альбомов\n",
    "    all_predictions = tabnet.predict(features.values)\n",
    "\n",
    "    # Усредняем предсказания понравившихся альбомов\n",
    "    avg_prediction = np.mean(all_predictions[liked_indices])\n",
    "\n",
    "    # Вычисляем \"схожесть\" как обратное расстояние до среднего\n",
    "    distances = np.abs(all_predictions - avg_prediction).flatten()\n",
    "    similarities = 1 / (1 + distances)  # Преобразуем расстояния в схожести\n",
    "\n",
    "    # Сортируем по схожести, исключая понравившиеся\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    recommended_indices = [i for i in sorted_indices\n",
    "                          if i not in liked_indices][:n_recommendations]\n",
    "\n",
    "    # Формируем результат\n",
    "    recommendations = data.iloc[recommended_indices][\n",
    "        [\"album_name\", \"artist\", \"genre\", \"popularity\"]]\n",
    "    return recommendations\n",
    "\n",
    "# Пример использования\n",
    "liked_albums = [\"Hurry Up Tomorrow\", \"Scorpion\", \"After Hours\"]\n",
    "recommendations = get_tabnet_recommendations(liked_albums)\n",
    "print(\"Рекомендуемые альбомы:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWaLBfp3wkja"
   },
   "source": [
    "# Метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBj1-juMwkjc"
   },
   "source": [
    "3. Cosine Similarity (Косинусное сходство)\n",
    "\n",
    "Оценивает, насколько схожи рекомендуемые альбомы с целевым альбомом.\n",
    "\n",
    "Чем выше значение (ближе к 1), тем лучше рекомендация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceJOf9jJwkjc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_scores = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYZrhp-Vwkje"
   },
   "source": [
    "4. Euclidean Distance (Евклидово расстояние)\n",
    "\n",
    "Оценивает \"расстояние\" между альбомами в векторном пространстве.\n",
    "\n",
    "Чем меньше расстояние, тем более схожи альбомы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56tl4zgwwkje"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "distances = euclidean_distances(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCfejY213pbT"
   },
   "source": [
    "5. Diversity\n",
    "\n",
    "Измеряет, насколько разнообразны рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXpZ2sKO3fm0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def diversity_score(dataset, user_likes, recommended_album, feature=\"genre\"):\n",
    "    # Получаем жанры понравившихся и рекомендованного альбомов\n",
    "    liked_genres = dataset.loc[user_likes, feature].values\n",
    "    recommended_genre = dataset.loc[recommended_album, feature]\n",
    "\n",
    "    # Кодируем жанры в бинарные векторы\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    all_genres = np.append(liked_genres, recommended_genre).reshape(-1, 1)\n",
    "    encoded = encoder.fit_transform(all_genres)\n",
    "\n",
    "    # Считаем косинусное сходство\n",
    "    avg_similarity = cosine_similarity(encoded[:-1], [encoded[-1]]).mean()\n",
    "    return 1 - avg_similarity  # Чем выше, тем разнообразнее\n",
    "\n",
    "# Пример:\n",
    "diversity = diversity_score(dataset, [\"Thriller\", \"Back in Black\"], \"Nevermind\")\n",
    "print(f\"Diversity: {diversity:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
